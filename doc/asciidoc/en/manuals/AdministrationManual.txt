// a2x: -L 
// a2x: --dblatex-opts "-d"
// a2x: --dblatex-opts "-p ./qvd.xsl"
// a2x: --dblatex-opts "-s ./AdministrationManual.sty"

The QVD Administration Manual
=============================
Rowan Puttergill <rowan.puttergill@qindel.com>
$Id: AdministrationManual.txt 11680 2011-07-22 08:22:43Z rputtergill $
:author initials: RP
:email: rowan.puttergill@qindel.com
:doctype: book
:adminmanual:
:toc:
:icons:
:numbered:
:qvdversion: 3.1
:website: http://theqvd.com/
ifdef::blogpost[]
// Use the same article for both a blog post and on the AsciiDoc
// website.
:blogpost-status: published
:blogpost-doctype: book
:blogpost-posttype: page
:blogpost-categories: doc
endif::blogpost[]

[preamble]
Revision Information
--------------------
This document was last updated: {revdate}

Its current revision number is set at: {revnumber} 

[preface]
Preface
========
This document will provide you with all of the information that you
need to install, manage and administer any of the QVD components
within a QVD solution. As an open-source product, QVD is constantly
growing and being improved. We endeavour to keep our documentation as
complete as possible and encourage readers to notify us of ways that
the documentation can be improved. If you have any queries or
suggestions, please email us at info@theqvd.com.

The document is broken into three main parts. The first part discusses
the core components that make up a solution, how they interact and how
they are installed and configured. The second part deals with
integration issues and how to tweak behaviours within QVD to achieve
better performance or to be more scalable. The third part is dedicated
to providing you with all of the information that you may need to
create and manage the Operating System Disk Images and Virtual Machines
that get loaded into each virtual desktop.

Additionally, we provide a Bibliography to reference external material
that may help you to gain a better understanding of the different
technologies involved in a QVD solution. We also provide a Glossary of
commonly used terms, that may help you to understand some of our own
terminologies and some less frequently encountered terms when you come
across them.

include::WhatIsQVD.txt[]

Some Notes About This Manual
----------------------------
In general, it is assumed that most users of the QVD will take advantage
of the KVM virtualization offered within the product. As a result, the majority
of this guide assumes that you will configure the product in this way. Where
users select to use LXC to achieve virtualization, there may be some differences
in configuration. Where these are significantly important, we have included 
information for both virtualization platforms. However, we have also included
a separate chapter on LXC virtualization which attempts to provide some additional
guidance to users who choose to explore this option.

In the same line, although we provide packages for other Linux distributions such
as SuSE Linux Enterprise Server (SLES), we assume that the majority of our users will
use Ubuntu Linux. As a result, many of the commands in this guide, along with the
locations of configuration files etc, are generally provided with the assumption
that you are using Ubuntu Linux. Where it is important the users of SLES are aware
of differences, we have attempted to also provide this information. A small chapter
is appended to the end of this guide to provide some further details for SLES users.

Core Components
===============
[partintro]
--
In this part of the manual, we discuss the core components that make
up a QVD Solution. We will explain the architecture of a solution, and
we will cover the installation and configuration settings specific to
each component in detail.
--

Components and Architecture
----------------------------

Introduction to QVD Components
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
QVD {qvdversion} is comprised of a number of core components that work
together to create a complete QVD solution. While not every single
component is necessarily required in order to create a functioning
environment, it is advisable that all components are actually installed to
ensure ease of management and to protect the stability of the platform.

There are three major server side components:

* QVD server,
* Administration Server, and
* PostgreSQL DBMS.

Ideally, each of these should be stored on a dedicated host for
stability reasons, although it will become clear later that these
components will have access to some shared resources in order to
function properly.

While it is likely that you will only have one Administration Server
and one PostgreSQL Database system in your environment, it is possible
to have any number of QVD Server Nodes. Therefore, most deployments of 
the QVD will also include the following components:

* Load Balancer
* Shared Storage Facility (e.g. NFS etc)

By using a Load Balancer in front of your QVD Server Nodes, clients 
connections can be balanced across healthy Server Nodes in order to access
a virtual desktop. This reduces the amount of configuration within the client
software and also ensures a much healthier environment to serve virtual desktops.

Since each server node will require access to particular shared resources, such
as the disk images that will be loaded into a virtual machine, and user home
data, a shared storage facility is generally set up in order to allow all of the
server nodes to have access to this shared data.

Within each Disk Image, that is loaded into a Virtual Machine out of which the
Virtual Desktop is served to an end user, there is an additional component which
becomes active for each Virtual Machine that is started:

* QVD Virtual Machine Agent

The QVD Virtual Machine Agent (VMA) is responsible for accepting connections from
the client via a QVD Server Node. It facilitates access to the desktop environment
running within the virtual machine, including the ability to configure printer access
and to configure the virtual machine to stream audio to the client.

Finally there is the client side component:

* QVD GUI Client

The client is packaged for a variety of Linux base operating systems,
and for Microsoft Windows. A client for Android platforms has also been released.

The client software can be installed on as many host systems as
required.

include::HighLevelArchitectureDiagrams.txt[]

QVD Architecture
~~~~~~~~~~~~~~~~
[[qvd_node_architecture]]
In most production environments, the architecture of a QVD environment
is such that several QVD Server Nodes will be running in parallel to
each other. The QVD environment is designed to handle a fully
load-balanced environment, so that you can have a High Availability
solution.

Internal Elements
^^^^^^^^^^^^^^^^^
A QVD Server Node is composed of two core elements:

* *L7R* - A Layer-7 Router that acts as the broker within the server
  environment, responsible for authenticating users, establishing
  sessions and routing connections to the appropriate virtual IP
  addresses. In general, the L7R is responsible for managing user
  status.
* *HKD* - A 'House Keeping Daemon' that tracks the status of virtual 
  machines. The HKD is responsible for starting and stopping virtual
  machines. The HKD monitors the health of each virtual machine and 
  then updates status information within the QVD Database, so that 
  other Nodes and the administration tools are able to function 
  accordingly. In general, the HKD is responsible for managing virtual
  machine status.

HKD Behaviour
^^^^^^^^^^^^^
The 'House Keeping Daemon' is responsible for managing virtual
machine states based on information that it detects within the QVD
Database. The HKD regularly polls the QVD database to determine the
status of each Virtual Machine. If the status has been changed by an
external element (such as the L7R or the QVD-WAT) the HKD is
responsible for enacting the appropriate commands to effect the status
change.

When QVD is configured for KVM virtualization, the HKD runs a KVM instance 
for each virtual machine that needs to be started, and provides startup 
options based on information obtained from the database.

When QVD is configured for LXC virtualization, the HKD will first check to
determine whether the image file has been uncompressed into the basefs folder
in the shared storage area, and uncompresses the image file if this has not
already been done. The HKD then uses the 'fuse-unionfs' module footnote:[<It is 
possible to use alternative methods to create a union style mount, including using
the *aufs* module which can offer significant performance improvements. Users have
also succeeded in using *bind mounts* to achieve the same purpose.>] to perform a
union mount of the image in the basefs folder with an automatically generated
overlay file system and home file system. This mount is performed inside the
rootfs folder in the shared storage. Finally, the HKD will load the newly mounted
image into an LXC instance.

As the Virtual Machine instance starts, the HKD will check that the image boots 
correctly, that it has network connectivity and that the QVD-VMA is running within 
the virtual machine. If any of these checks fails, the HKD will change the state of 
the virtual machine to 'blocked' within the QVD Database. After a short period, the 
HKD will kill the running virtual machine.

During each loop run that the HKD performs it will check the health
of all running virtual machines, it checks the database to determine
if there are any VM state changes, implements any changes to VM state,
and updates information in the database pertaining to VM state.

QVD Client and L7R Server Node Interactions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The QVD Client connects directly to the L7R element of a QVD Server
Node. The Client initiates a connection over HTTPS, where it is
prompted to provide HTTP BASIC authentication credentials.·

The L7R will then connect to the backend database to determine how
authentication should take place (i.e. locally or using an external
LDAP directory) and take the appropriate steps to perform the
authentication process. The L7R will return an HTTP OK response if the
authentication was successful, or will return a 401 Unauthorized if
authentication fails.

Once authenticated, the client requests a list of virtual machines that·
are available to the user. The server responds with a JSON formatted list·
of virtual machine IDs and their corresponding status. The client selects
an appropriate virtual machine to connect to and submits a GET request
with the ID of the virtual machine at a standard GET variable. It also
requests a protocol upgrade to QVD/1.0 within the HTTP request
headers.

The L7R performs the necessary steps to ensure that the virtual machine·
is up and waiting for connections using the NX protocol. If the
virtual machine is not running on any server node, it will determine
which node it should be started on and automatically start a virtual
machine for that user. In all events, the L7R will determine which
node is running the virtual machine and will forward all requests to
this machine for all further handling, including checking to see that
an NX session can be set up. During this process, the L7R will return a series 
of HTTP 102 responses indicating the progress of the processing required to 
establish a connection with the Virtual Machine. If the virtual machine is 
available, the L7R establishes a connection to the *nxagent* running on the 
virtual machine and becomes a pass-thru proxy for the NX session. Once the 
session is set up, the L7R will issue a final HTTP 101 (Switching Protocols) 
response to the client, and the protocol for all future interactions with the 
client will be upgraded to the NX protocol, secured using SSL. The L7R
updates the QVD Database to set the status for the virtual machine to
indicate that a client is connected.

From this point onward, all communications between the client and the
Virtual Machine are performed over the NX protocol via the L7R. When
the client disconnects, the L7R updates the QVD Database to represent·
the change in virtual machine status.

The process flow is indicated in the following diagram:

.Protocols and process flow for Client/Server Node interaction
image::../images/client_server_protocol.png[alt="Protocols and process flow for Client/Server Node interaction" width=98%]

L7R in an HA load-balanced environment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
As already mentioned, Server Nodes are designed for a fully
load-balanced environment. In order to cater for this, the L7R element
of each Server Node is capable of redirecting traffic for a particular
virtual machine to any other Server Node in the environment.

The usual configuration is such that a Virtual Machine is started for
each user on any one of the server nodes. When a user is authenticated
by any L7R within the solution, the L7R determines which server node
is currently running a virtual machine for the authenticated user.
This is achieved by querying the QVD Database. If a running virtual
machine is detected within the environment, the L7R will reroute all
traffic for that connection to the appropriate server node.

If no virtual machine is currently running for the user, the L7R makes
use of an internal algorithm to determine the most appropriate node to
start a new virtual machine for the user. This algorithm is based on
assessing which node has the highest quantity of free resources,
calculated as the weighted sum of free RAM, unused CPU, and a
random number to bring some entropy to the result. 

When an appropriate node has been selected, the database is updated so
that a virtual machine will be started by the HKD on the correct host. The 
L7R will then reroute all traffic for that connection to the server node 
that has been selected to run the new virtual machine.

Virtualization Technologies
^^^^^^^^^^^^^^^^^^^^^^^^^^^
QVD supports two different virtualization technologies: KVM (Kernel Virtual Machine)
and LXC (Linux Containers). Each virtualization technology comes with its own set
of advantages and will prove more useful for particular use cases. Therefore, a good
understanding of your own requirements and an understanding of these two technologies
will help you to determine how to configure your QVD deployment.

KVM Virtualization
++++++++++++++++++

The Kernel Virtual Machine (KVM) is a fully featured hypervisor 
that runs inside of the kernel of the linux host operating system. The hypervisor ensures
absolute separation from the underlying host operating system, allowing you to load completely
different operating systems and distributions into each virtual machine and expect them to
function just as if they were running on completely separate hardware.

While there is some debate over whether KVM is actually a Type-1 bare-metal hypervisor, since it does require
the linux Kernel in order to function, most virtualization experts agree that combined with 
the linux Kernel, KVM functions in exactly the same way as any other bare-metal hypervisor,
such as Xen or VMware's ESXi. In fact, in the recently published SPECvirt 2011 benchmark reports, KVM
came second in performance only to VMWare ESX, indicating a high level of viability as a commercial-grade
virtualization platform.

Since KVM uses absolute separation, it is much easier to configure and manage than LXC. However,
although it offers competitive performance to other hardware hypervisors, each virtual machine is
necessarily running its own kernel. Resources need to be dedicated to each virtual machine, whether they
are being used or not. In this way, KVM is not as efficient as LXC, but offers much greater flexibility
and ease of management.

LXC Virtualization
++++++++++++++++++

Linux Containers (LXC) provide Operating system-level virtualization.
In this way, they act as an alternative to the full hardware-level
virtualization provided by the KVM hypervisor. LXC behaves in a
similar manner to a chrooted environment within Linux, but offers a
greater level of isolation and management of resources between 
containers through the use of 'namespaces' and 'cgroups'. For
instance process IDs (PIDs), network resources and mounts for each
container will be isolated from other containers and can be logically
grouped together to apply resource management rules and other policies
specific to the container. This allows you to gain many virtualization
benefits while keeping down overall resource requirements and by
re-using the same kernel across virtual machines. LXC is fully
supported by the Linux Kernel, and has been included in QVD since
version 3.1.

Virtual Machines and VMA
^^^^^^^^^^^^^^^^^^^^^^^^
Virtual Machines are started by the HKD on a per-user basis. In a
production environment, it is usual for there to be a number of
different QVD Node Servers running in parallel. Virtual Machines are
started for different users across the different QVD Server Nodes, so
that there is one virtual machine instance running for each user that
needs to be provisioned. If a virtual machine has not been started for
a user, and the user connects and authenticates against an L7R, the
L7R will use its load-balancing algorithm to determine which node
should run the user's virtual machine and the database will be updated
so that the virtual machine will be started on the appropriate node.

When Virtual Machines are started, they load an "Operating System Flavour" 
or OSF. The parameters for the Virtual Machine are determined by data stored 
within the QVD-DB for each OSF. In general, the OSF's Disk Image is loaded 
from a network share. There is a separate chapter within this document dedicated 
to creating, editing and managing OSFs.

Virtual Machines make use of _overlays_ in order to best utilize
different elements of the Guest operating system, and in order to make
particular elements persistent. For instance, while write activity is
not persistent within the actual OSF that is loaded, it is important
that data written to the user's home folder or desktop is stored for
future connections to the virtual desktop. 

Within instances of QVD that make use of KVM virtualization, this is achieved
by storing the user's home directory within a 'qcow2' image. This is loaded 
over the home directory within the OSF that is running in the Virtual Machine. 

In instances of QVD that make use of LXC virtualization, this is achieved by
taking advantage of _unionfs_ mounts footnote:[<Once again, note that it is possible to use 
alternative methods to create a union style mount, including using
the *aufs* module or by simply using *bind mounts* to achieve the same purpose.>]. 
The user's home data and any overlay data is stored within a separate directory outside of the image used for a
virtual machine. These folders can then be mounted over the base image at
runtime, in order to create a container specific to each user and virtual
machine.

The 'qcow2' image or user home directory is usually stored on a network share, 
so that it is accessible  to any server node within the environment. If the user's 
virtual machine is later started on a different Server Node, the user's home directory 
can be loaded at run time and the user's modified data will always be available to the
user. 'Overlays' can also be used to make other data such as log and tmp 
files persistent from a user perspective.

Depending on the virtualization technology configured within QVD, virtual machines
will be started either using KVM or LXC. However, it is important to understand
that the images for these two different technologies are very different and it
is not possible to switch between virtualization technologies.

Once running, each Virtual Machine must load the QVD-VMA (Virtual Machine Agent) in
order to function properly. The VMA will ensure that the *nxagent* is
available so that a client is able to connect to the virtual desktop
that is created for the user. It also returns different states that
helps the L7R to determine user state, which can be fed back to the
QVD-DB. When an OSF is created, it is fundamentally important that the
QVD-VMA is installed and configured in order for QVD to work at all.

QVD Administration
~~~~~~~~~~~~~~~~~~
QVD Administration can be performed using one of two tools:

* *qvdadmin.pl*: a command line utility that can be installed on any
  machine that has the connectivity to the QVD-DB and that is
  configured appropriately for this purpose.

* *QVD-WAT*: a Web-based Administration Tool that allows an
  Administrator to remotely access the solution and to perform a
  variety of administrative tasks using a standard web browser.

Both tools require access to the QVD-DB and will need to be configured
for this purpose. Nearly all of the commands that can be performed
through either of these tools will simply change values for entities
within the QVD-DB. Actions are then carried out by the various QVD
Server Node elements based on the changes made within the QVD-DB.

The QVD Administration tools are also be used to load new images into
QVD and to configure their runtime parameters. In order to facilitate
this functionality, these tools need access to the folders where these
images are stored and accessed by the Virtual Server Nodes. Usually,
this access is provisioned over a network file share such as NFS.

Base QVD Configuration
----------------------
include::QVD_CONFIG.txt[]

QVD-DB
------
include::QVD_DB.txt[]

QVD Web Administration Tool
---------------------------
include::QVD_WAT.txt[]

QVD Server Nodes
----------------
include::QVD_NODE.txt[]

QVD CLI Administration Utility
------------------------------
include::QVD_CLI.txt[]

QVD GUI Client
--------------
include::QVD_CLIENT.txt[]


Design Considerations and Integration
=====================================
[partintro]
--
In this part of the manual, we explore things that will affect the
design of your solution, such as your virtualization technologies, 
storage requirements and authentication mechanisms.
--

Shared Storage
--------------
Since there are multiple server-side components within the QVD
infrastructure, and each of these will usually be installed on a
number of different physical systems, it is important to set up some
shared storage facility that is accessible to all of the hosts within
your server farm.

.Shared Storage is accessed by Node Servers and the QVD WAT
image::../images/shared_storage_access.png[alt="Shared Storage is accessed by Node Servers and the QVD WAT", width="250"]

The currently supported network file sharing services are GFS or OCFS2 
on top of some SAN server (i.e. iSCSI, AoE, etc.) and NFS.

QVD usually keeps all commonly used files in the directory location:

    /var/lib/qvd/storage

NOTE: All of the paths used by QVD are configurable items, so you
should keep in mind that although this is the default location, the
pathnames within your infrastructure may be different depending on
your configuration. You can check these configuration settings using 
the QVD CLI Administration Utility.

While some of the components do not need access to all of the QVD
Storage folders, and in some cases you can opt to have some of these
folders running locally on one system, we recommend that all of these
folders are accessible within some form of network based shared storage.

TIP: Backing up your the QVD folders that are kept in your Shared Storage facility
is highly recommended. At a minimum, you should ensure that the folders where
your user home data is stored and the folders where your Disk Images are stored
are backed up in line with your disaster recovery strategy.


Storage Folders
~~~~~~~~~~~~~~~
[[storage_folders]]
There are a variety of folders that belong in the storage path. Many of
these are specific to the type of virtualization that you choose to make
use of within your environment.

General Storage
^^^^^^^^^^^^^^^

* *staging*: temporary location for all DIs that you want available in
  the QVD-WAT for the purpose of loading as an image. Files located
  here are available within QVD-WAT when you select to add an image.
  The image file will be copied out of this directory and into the
  *images* folder when it is enabled using one of the administration tools. 
  The staging folder can either be hosted locally or on a network share, 
  but must be accessible to the QVD-WAT.
* *images*: location of the DIs (Disk Images) that are
  loaded by the nodes for each Virtual Machine that is created. These
  need to be accessible to QVD Server Nodes and to the QVD-WAT. This
  directory might be stored on a network share, but in a very simple
  configuration where the QVD-WAT is either not used or is hosted on
  the same system as the QVD Server Node, it can be hosted locally
  which will help to improve performance. Note that where KVM 
  virtualization is used, the image is loaded into the virtual machine
  from this directory. When LXC virtualization is used, the image is
  extracted from this directory into the *basefs* directory, before it
  is loaded.


KVM Storage Directories
^^^^^^^^^^^^^^^^^^^^^^^

* *homes*: location of user home data. Under KVM, home data
  is stored in individual files as qcow2 images. The *homes* directory 
  should be accessible to all QVD Server Nodes usually on some type of 
  network file share such as NFS, OCFS2 or GFS2.
* *overlays*: location used to store overlays for data that is constantly
  written to the Operating System in order for it to function correctly, such
  as temporary files and variable data etc. Usually this folder can be hosted 
  locally, but for more persistent behaviour in your virtual machines, you can 
  choose to store these on a network share and configure QVD to make your 
  virtual machines persistent.

LXC Storage Directories
^^^^^^^^^^^^^^^^^^^^^^^

* *basefs*:  location of the DIs (Disk Images) that are
  loaded by the nodes for each Virtual Machine that is created. These
  need to be accessible to QVD Server Nodes and to the QVD-WAT. This
  directory might be stored on an network share, but in a very simple
  configuration where the QVD-WAT is either not used or is hosted on
  the same system as the QVD Server Node, it can be hosted locally
  which will help to improve performance. The basefs folder will contain
  a subdirectory for each DI, which will in turn contain the complete 
  filesystem tree for a functioning operating system
* *homesfs*: location of user home data. Under LXC, home data
  is stored within subdirectories inside the *homesfs* directory, named 
  according to the user-id and the osf-id stored within the QVD-DB. 
  The *homesfs* directory should be accessible to all QVD Server Nodes 
  usually on some type of network file share such as NFS, OCFS2 or GFS2.
* *overlayfs*: location used to store overlays for data that is constantly
  written to the Operating System in order for it to function correctly, such
  as temporary files and variable data etc. Usually this folder can be hosted 
  locally, but for more persistent behaviour in your virtual machines, you can 
  choose to store these on a network share and configure QVD to make your 
  virtual machines persistent.
* *rootfs*: location of the running LXC once all required mountpoints have
  been mounted and configured. Usually this folder is local to each QVD
  Node Server, for performance, but it could equally be stored within the
  shared storage space.


NFS
~~~
In this section of the document we will provide instructions for setting up NFS for QVD, as
this is one of the more commonly used network file sharing protocols used to facilitate shared
storage. We will assume that you are installing and configuring NFS on Ubuntu 10.04 (Lucid Lynx) 
in order to maintain consistency in our instruction set, however you should be able to extrapolate these
instructions to provide NFS access for any distribution.

TIP: We recommend that you run through the following process before
installing any QVD Server components to ensure that when the QVD
components are installed, they are automatically making use of the NFS
share from the beginning. This way, you are less likely to run into
trouble migrating files and creating directories in the longer term.

Installing the NFS Server
^^^^^^^^^^^^^^^^^^^^^^^^^

First install the NFS Server:

----
# apt-get install nfs-kernel-server
----

Configuring the NFS Server
^^^^^^^^^^^^^^^^^^^^^^^^^^

Add an entry to /etc/exports as follows:

----
/var/lib/exports        *(rw,sync,no_subtree_check,no_root_squash)
----

Note that this would mean that on your NFS Server, you would set up
each of the QVD storage directories within the path
`/var/lib/exports`. You can choose an appropriate location if you
would prefer to host these files at an alternative path.

Once you have added your path entry within the NFS Server's exports,
you should reload the NFS Server.

----
# /etc/init.d/nfs-kernel-server reload
----

The NFS Server should now be making the configured path available over
the network.

Mounting the NFS directory on QVD Hosts
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Each host system that is running any QVD server component will now
need to be configured to access the NFS share that we have configured
on the NFS server.

First, create the mountpoint on your host systems:

----
# mkdir -p /var/lib/qvd/storage
----

Ensure that you have the tools required to access an NFS share
installed on your host systems:

----
# apt-get install nfs-common
----

To ensure that the NFS file system is always mounted at boot time,
edit your `/etc/fstab` to add the following line:

----
aguila:/var/lib/exports /var/lib/qvd/storage      nfs rw,soft,intr,rsize=8192,wsize=8192  0       0
----

Note that in the line above 'aguila' is the name of the server hosting
the NFS Share. You should substitute this with the IP address or
resolvable hostname of your NFS Server.


Once you have finished editing your fstab, you should be able to mount
the NFS export on your host systems:

----
# mount /var/lib/qvd/storage
----

Finally, you should check that the NFS export has been properly
mounted. You can do this by running the `mount` command and then
checking the output to see that your NFS export is listed:

----
# mount
...
aguila:/var/lib/exports on /var/lib/qvd/storage type nfs (rw,soft,intr,rsize=8192,wsize=8192,addr=172.20.64.22)
----

LXC Virtualization inside QVD
-----------------------------
include::UsingLXC.txt[]

Authentication
--------------
Although QVD does provide its own authentication framework, which
stores its users within the QVD database, it is quite common to
require integration with another authentication framework so that
changes to user passwords etc, do not need to be replicated within
the QVD-DB.

QVD does provide a certain level of integration with external
resources. In this chapter, we will explore two of the more common
integration requirements, and the level of support offered by QVD for
these authentication frameworks.

LDAP Integration
~~~~~~~~~~~~~~~~
The most commonly used authentication framework is LDAP, and QVD
provides support to authenticate against LDAP right out of the box.

Configuring QVD for LDAP Authentication
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
LDAP Authentication can be configured within QVD by setting a few
configuration keys in the QVD database. This can be achieved using the
<<qvd-admin-cli,QVD CLI Administration Utility>>.

----
# qvd-admin.pl config set l7r.auth.mode=ldap
# qvd-admin.pl config set l7r.auth.ldap.host=aguila:3389 
# qvd-admin.pl config set l7r.auth.ldap.base=dc=example,dc=com
----

In the above example, we have changed the QVD authentication mode to
LDAP, we have set the LDAP Host and port number to `aquila` on port
`3389`. And we have set the LDAP base DN that should be searched for
matching users to `dc=example,dc=com`.

With these basic configuration elements, QVD will automatically search
the LDAP directory for a matching username, and then perform a BIND
against that user using the credentials supplied by the client.
By default, the search is performed with a scope set to 'base' and the
filter set to '(uid=%u)'. Using our example host above, a client
connecting with the Username set to 'guest' would need a corresponding
entry 'uid=guest,dc=example,dc=com' within the LDAP server running on
host 'aquila' available on port '3389'.

It is possible to change the scope and filter settings for the search,
to allow QVD to scan other branches and attributes to find a matching
user:

----
# qvd-admin.pl config set l7r.auth.ldap.scope=sub
# qvd-admin.pl config set l7r.auth.ldap.filter=(|(uid=%u)(cn=%u))
----

The above examples change the default search scope for LDAP
authentication to 'sub' and the filter will cause the search to match
users with either the 'uid' or the 'cn' equal to the provided
Username.

QVD LDAP Limitations
^^^^^^^^^^^^^^^^^^^^
While it is relatively trivial to get QVD to authenticate users
against LDAP, you will still need to create matching users within your
QVD-DB in order to assign virtual machines to them. Alternatively, you
can make use of the ~Auto Plugin~ discussed in the next section of this
chapter, to automatically handle user provisioning.

QVD tools allowing you to change user passwords within QVD will not
update passwords within an LDAP backend, as this may affect the
functioning of other facilities within your infrastructure. While
these tools will report success for a password change, it is important
to understand that the password that has been changed is the one
stored within QVD-DB for the user, and not the password within the
LDAP directory. If you are making use of LDAP Authentication, all
password changes should be made using the tools that you usually make
use of to manage your users.

Automatic User Provisioning Plugin
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
When using an alternative authentication mechanism, such as the LDAP
authentication plugin, there is a common requirement to automatically
provision users who have authenticated against the provided plugin. The
~Auto Plugin~ is designed to cater to this requirement.

When QVD is configured to authenticate against an external source, such as LDAP,
QVD usually does not have have any record for the users that log in. When the 
~Auto Plugin~ is enabled, the user record is created automatically. User records
created this way are provided with a default virtual machine, as specified in the
plugin configuration.

To enable the plugin, add "auto" to the list of enabled plugins in the L7R configuration:

----
qvd-admin.pl config set l7r.auth.plugins=auto,ldap
----

Note that in the example above, we are using the auto plugin in conjunction with the ldap
authentication plugin.

You will now need to tell the auto plugin which OSF to use as the default for provisioning
a virtual machine for the user:

----
qvd-admin.pl config set auth.auto.osf_id=1
----

In this case, new users who are authenticated using LDAP will automatically be provisioned with
a user account and a VM will be created using the OSF ID that we have specified.

It is also possible to force the use of a particular Disk Image using the <<di_tag,DI Tag>> functionality.
This can be done by setting the following configuration option:

----
qvd-admin.pl config set auth.auto.di_tag="testing"
----

A typical example to use the LDAP Plugin and the Auto Plugin in conjunction would require you
to run the following commands:

----
qvd-admin.pl config set l7r.auth.plugins=auto,ldap
qvd-admin.pl config set auth.ldap.host=ldaps://myldap.mydomain.com:1636
qvd-admin.pl config set auth.ldap.base=ou=People,dc=example,dc=com
qvd-admin.pl config set auth.ldap.scope=sub
qvd-admin.pl config set auth.ldap.filter=(&(objectClass=inetOrgPerson)(cn=%u))
qvd-admin.pl config set auth.auto.osf_id=1
----

This would mean that as users authenticated for the first time, using QVD, they would be
authenticated using your LDAP repository and they would be automatically provisioned with
a default desktop to start work immediately.

OpenSSO and OpenAM
~~~~~~~~~~~~~~~~~~
QVD is capable of providing support for other authentication platforms
such as the federation and access management framework developed by
Sun Microsystems and known as OpenSSO.

NOTE: Since Oracle's acquisition of Sun in 2010, OpenSSO has been
discontinued, but a fork known as OpenAM is available from ForgeRock.

In order for OpenSSO or OpenAM to function within QVD, the appropriate
plugin should be installed onto all of your QVD Server Nodes and you
will need to configure the VMA within each of your OSFs where you want 
to provide federation support.

Currently, this is an advanced configuration option that will require
some professional help to implement. Customers who choose to pay for
support can be provided with instructions and help deploying this
facility.

Load Balancing
--------------

Introduction
~~~~~~~~~~~~
QVD is designed to be used in a load-balanced environment. Since a
typical deployment makes use of several QVD Server Nodes to run all of
the virtual machines, it is common to have Clients connect to these
through a hardware load balancer.Since a virtual machine could run on 
any single Server Node, and a client could connect to any other Server Node, 
QVD's L7R component will handle the forwarding of a connection to the correct 
Server Node. However, since each Server Node has limited resources,
running virtual machines need to be equitably distributed to maximize
system resources across the Server Node cluster.

If a Virtual Machine is not already running for a connecting user, the
L7R component will determine which Server Node would be most
appropriate to use in order to start a new virtual machine for that
user. QVD uses its own load balancing algorithm to determine which
node should be used for the new virtual machine. This algorithm
assesses which node has the highest quantity of free resources,
calculated as the weighted sum of free RAM, the weighted sum of unused
CPU, and a weighted random value to bring some entropy to the result.
Once the best candidate Server Node has been selected, the QVD-DB is
updated to indicate that the virtual machine should be started on this
Server Node, and the virtual machine is automatically started by the
Server Node's HKD.

This whole process is known as QVD Load Balancing, and it is used to
ensure that running virtual machines are equitably distributed across all of
the Server Nodes. This maximizes the resources available to any
virtual machine to preserve healthy functionality.

QVD Health Checking
~~~~~~~~~~~~~~~~~~~
When using an external load balancer to route traffic to the different QVD Server
Nodes, you will generally need some method to perform health-checking against
each of the L7R instances. The QVD L7R daemon includes a simple health checking
facility that responds over HTTP.

Typically, you will need to configure your load balancer to perform an HTTP GET on
the URL: https://hostname/qvd/ping where 'hostname' is the hostname or IP address
for the Server Node instance. The query will return a text string with the content
"I am alive!" if the server is healthy and available.
  
Some of our users take advantage of the software load balancing provided by Linux Virtual 
Server (LVS). An example of the configuration required within the ~ldirectord.cf~ file follows:

----
autoreload = no
checkinterval = 1
checktimeout = 3
negotiatetimeout = 3
logfile="/var/log/ldirectord.log"
quiescent = yes
virtual = 150.210.0.72:8443
    checkport = 8443
    checktype = negotiate
    httpmethod = GET
    protocol = tcp
    real = 150.210.4.1:8443 gate 10
    real = 150.210.4.2:8443 gate 10
    receive = "I am alive!"
    request = "/qvd/ping"
    scheduler = wlc
    service = https
----

Changing the weighting in the QVD Load Balancer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The default QVD Load Balancing algorithm calculates the current system
load for each of the available server nodes by multiplying the
available RAM, available CPU and a random number in order to score each
system in the cluster. As we have already mentioned, these figures are
weighted, so that you can alter how the load balancer functions.

Increasing the weight on the RAM variable in the algorithm will cause
the load balancer to add precedence to systems with more available
RAM.

Increasing the weight on the CPU variable in the algorithm will cause
the load balancer to add precedence to systems with more available 
CPU.

Increasing the weight on the random variable in the algorithm will cause
the load balancer to increase the likelihood that a more random server
node will be selected.

These weights are controlled as configuration settings within QVD-DB,
and can be altered using the QVD CLI Administration Utility:

----
# qvd-admin.pl config set l7r.loadbalancer.plugin.default.weight.cpu=3
# qvd-admin.pl config set l7r.loadbalancer.plugin.default.weight.ram=2
# qvd-admin.pl config set l7r.loadbalancer.plugin.default.weight.random=1
----

In the above example, we have assigned more weight to CPU resources,
slightly less to RAM, and even less to the randomizer. This will
result in new virtual machines being started on the Server Nodes that
tend to have more CPU resources available.

Building a Custom QVD Load Balancer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Since QVD is an open-source product built largely in Perl, it is
relatively simple to build your own customized QVD Load Balancer that
uses an alternate algorithm. A typical use case would be where you
have a dedicated set of Server Nodes that you would prefer to use over
another set.

QVD has a plugin system for load balancers. A load balancer plugin is
a subclass of QVD::L7R::LoadBalancer::Plugin that has to be within the
package QVD::L7R::LoadBalancer::Plugin.

Plugin API
^^^^^^^^^^

get_free_host($vm) = $host_id
+++++++++++++++++++++++++++++

Return the id of the node on which the virtual machine $vm should be
started. A load balancer has to implement at least this method.

The parameter $vm is QVD::DB::Result::VM object. It gives you access
to the virtual machine's attributes and properties. The attributes and
properties of the VM's user and OSF can be accessed through $vm->user
and $vm->osf respectively. Other data can be accessed through QVD::DB.

init()
++++++

Initialize the load balancer. Use this if your load balancer has to be
step up, for example by loading a persistent cache.

Minimal example: random assignment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This load balancer assigns virtual machines to random backend nodes.

[source,perl]
----
package QVD::L7R::LoadBalancer::Plugin::Random;

use QVD::DB::Simple;
use parent 'QVD::L7R::LoadBalancer::Plugin';

sub get_free_host {
    my ($self, $vm) = @_;
    my $conditions = { backend => 'true',
                       blocked => 'false',
                       state   => 'running' };

    my $attr = { columns  => 'host_id' };

    my @hosts = rs(Host)->search_related('runtime', $conditions, $attr)->all;
    return $hosts[rand @hosts]->host_id;
}

1;
----

Operating System Flavours and Virtual Machines
==============================================
[[osf_and_vm]]
[partintro]
--
In this part of the manual, you will find all of the information that
you need to create, edit and manage an Operating System Flavour (OSF)
that will get loaded into your Virtual Machines. We also explore the
Virtual Machine Agent in a little more detail to see how you can use
it to trigger your own functionalities based on actions performed by
users accessing the Virtual Machine.
--

DI Creation
-----------

Introduction
~~~~~~~~~~~~
An OSF (Operating System Flavour) is actually composed of two elements:
a DI (Disk Image), and some runtime parameters stored within the QVD-DB
when the Disk Image is loaded into QVD using QVD-WAT or the QVD CLI
Administration Utility. In this chapter, we will concern ourselves
largely with the actual Disk Image part of the OSF, since the runtime
parameters are covered in the other relevant chapters.

QVD uses DIs in order to serve groups of users that make use of a
common set of applications. By using a single image to cater to a
number of users, it becomes easier to administer desktop environments
for all of your users. It also improves overall security, since a
policy can be applied to each group of users.

In this way, if a group of users require a particular application, you
can install it once and the change will apply to all of the users that
share the same DI. Equally, you can remove an application from an
entire group's desktop environment.

DIs can easily be duplicated, so that you can quickly create
additional environments for different subsets of users. By copying a
base image, you can edit the copy and provide additional applications
or other customizations to a second set of users without having to repeat 
a full operating system installation.

In this way QVD can massively reduce administration and maintenance,
improve desktop conformity, and ease security policy implementation.

In this chapter, we will look at the process involved in creating your
own base Disk Image for an OSF, so that you can implement an
environment that is perfectly suited to your users.

System Requirements for KVM Images
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
You can create a DI that can be loaded into QVD on any Linux system
that you have available for the task. In order to create an image, you
will need to meet the following base requirements:

* Server x86 with virtualization extensions (Intel or AMD).
* Linux Operating System (preferably Ubuntu, in order to conform with
  these instructions)
* At least 10 GB free disk space
* qemu-kvm installed
* An Ubuntu Desktop installation ISO to be used for your Guest
  operating system

Creating a KVM Image
~~~~~~~~~~~~~~~~~~~~
In order to create your DI, you will need to download the installer
of the guest operating system that you intend to serve to your users.
Since the DI will need to support the QVD VMA we recommend that you
use an Ubuntu variant as your choice of operating system, since it
will prove easier for you to set up and configure this environment.
It is possible to make use of any other Linux-based operating system
but you may need professional help creating a functional image.

For customized packages, we recommend that you install Ubuntu using
the 'Desktop alternate installer'. You will need to visit
http://www.ubuntu.com/download/ubuntu/alternative-download in order to
find the installation ISO that you should download.

Create a QCOW2 File
^^^^^^^^^^^^^^^^^^^
QVD will make use of the qcow2 disk image format to create a
virtual disk that will be used to store the DI. This virtual disk
file is essentially the base hard disk used within each virtual machine.
In order to create the qcow2 disk image file, you can run the
following command:

----
# kvm-img create -f qcow2 example.img 4G
----

In the command, the file that will be created will be called
`example.img` and will have a virtual hard disk size of a maximum of 4GB. In
actuality, the qcow2 will only create a relatively small image file.
One of the features of the qcow2 format is that it can expand the
image as required. the 4GB limit is applied to prevent the image from
growing too large without any control.

TIP: Remember that user's home directories will not be stored within this
image, so the limit to the image file size will not affect user home
space.

Installing the Operating System
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Once you have created the virtual disk file that you will install the
base Operating System into, you need to load it into a virtual machine
and boot the Ubuntu CD installer image within the virtual machine. You
can do this by running `kvm` in the following way:

----
kvm -hda example.img -cdrom ubuntu-10.10-alternate-i386.iso -m 512
----

In the example command above, the `ubuntu-10.10-alternate-i386.iso` is
the installer ISO that you would have downloaded from the Ubuntu
website.

KVM should load a basic virtual machine that will boot the installer
ISO. You should be able to follow the instructions in the installer to
complete a standard Ubuntu installation within the virtual machine.
You will notice that you are installing into the virtual hard disk
which is only 4GB in size. During the installation, you will be
prompted for a username and password. You should select a username and
password that you will use to manage package installation and other
administrative tasks within the DI at a future date.

When you have completed the installation, the installer will prompt
you to reboot. You should allow the installer to reboot and KVM will
restart the virtual machine, this time booting off the virtual hard
disk and loading your newly installed Operating System.

include::QVD_VMA_CONFIG.txt[]

Creating and Configuring LXC Disk Images
----------------------------------------
[[LXC_DI_Creation]]
The process involved in creating an LXC Disk Image very much depends on the 
operating system or distribution that you wish to run. However, there are a
number of common essential guidelines that should be followed in order to 
create a functional image. In this document, we will present a rough outline
of the steps that need to be taken, and will then provide some base examples
of how this is done for Ubuntu and for SuSE systems.

The following list provides a basic outline of the steps that need to be 
taken to prepare an image in order to have it run within the QVD:

* Install a copy of the base operating system into a file-space. On Debian-based 
systems, this can be achieved using 'debootstrap', while on SuSE systems, 'zypper'
can be used. It is also possible to simply use the filesystem created during an 
original installation.
* Manually recreate the device nodes in /dev
* Remove any hardware references in your init scripts. This process is very 
specific to the operating system that you are using.
* Add the QVD sources to your repositories, and add the 'qvd-vma' package along with its
dependencies. This can be done in a number of ways including a simple 'chroot', running
the container within lxc, or using `zypper --root` within SLES environments.
* Update the system inittab
* Create an fstab

Typically, most Linux distributions provide 'templates' that can be used to automate the
steps required to build a base installation within a container. These templates are 
essentially shell scripts that automate many of the steps required to install the base
operating system, to remove hardware references in the init scripts and to create device
nodes. These scripts are usually stored in '/usr/lib/lxc/templates/', but you should not
need to modify them. Instead, you can usually invoke the template that you wish to use by
running the `lxc-create` command with the '-t' switch:

----
# lxc-create -n MyUbuntuContainer -t ubuntu
# lxc-create -n MySLESContainer -t sles
----

NOTE: The template for SLES linux is only provided with SLES 11 SP2 and up. In fact, due to
many of the complications in creating functional containers on earlier versions of SLES, we
recommend that LXC under QVD is only used on these versions of SLES. Even when using this
version of SLES, you will be required to perform a number of modifications to the default template.

We suggest that you take the time to look over the template scripts to understand what they
are doing. During some installations, some modification to the template script may be required
and it is important that you know what the script is trying to achieve. The list of steps presented
above should provide some outline for the majority of work handled by a template script.

If you take advantage of the lxc-create command, as specified above, you will be notified that
you have not provided a configuration file. This is not extremely important, since QVD will
actually generate configuration files for your containers as required. However, if you intend
to use a container outside of QVD, you may need to create a configuration for your container
after it has been created. Some instruction on how to do this is presented for each distribution
in the subsections below.

Remember that if you use the lxc-create command, the container that is created will only include
the packages required for a minimal installation of the distribution that you are creating. Once
this container has been created, you will still be required to install a wide variety of 
applications in order to set up a functional desktop environment.

NOTE: Some of the more feature rich desktop environments, such as Gnome and KDE are often packaged
in such a way that their dependencies include packages that directly access hardware. This can cause
trouble when trying to set up an LXC image. Therefore, we recommend that when using LXC virtualization
you take advantage of one of the more lightweight desktop environments, such as LXDE or Razor-QT. While
it is possible to use Gnome or KDE, you will need to be particularly experienced at package management
and in your configuration capabilities. In our examples and demonstration images, we generally use LXDE or
Razor-QT to minimize configuration issues.

Ubuntu Image
~~~~~~~~~~~~
The following outline should guide you through preparing and setting up an LXC image
on an Ubuntu system.

Setting up and configuring a base LXC image on Ubuntu is relatively easy if you take 
advantage of the template that is packaged with LXC. To get started, use the lxc-create 
command to build a simple container:

----
# lxc-create -n MyUbuntuContainer -t ubuntu
----

This will invoke the debootstrap command to download a base Ubuntu installation, and will 
perform some basic tasks such as the recreation of your device nodes. Although you will be
notified that no configuration file has been provided, a very basic configuration will be created for 
you by the template script, but will not include any networking options. This isn't a problem as QVD will
handle your configuration parameters when the image is loaded into the QVD framework, and the container
will automatically share the network resources of the host system if a network configuration has not been
provided. Once the command has finished running, the image and its configuration file will be stored in 
/var/lib/lxc under a directory that matches the name that you provided following the -n switch.
 
A container created using the default template is not in any state that can be used within the QVD framework.
To begin with, it will not have X Windows installed and it will not have any desktop environment available.
It is possible to build a simple container and then to chroot into it to install the packages required for
a desktop environment by doing the following:

----
# chroot /var/lib/lxc/MyUbuntuContainer/rootfs/
# apt-get install lubuntu-desktop
----

This may help you to get to a point where you can continue to work toward building your disk image, but
the preferred approach is for you to edit the template script before you create your container. Ideally,
your template script should at least include the commands that should be used to handle the installation
of packages for your desktop environment as well as the QVD VMA packages. The QVD team maintains its own 
set of modified template scripts that will ensure that a desktop environment and the necessary QVD VMA 
packages are also installed. These templates are not supported, but may help you to get started. Template 
scripts are usually installed in /usr/lib/lxc/templates.

If you are using the 'lxc-ubuntu' template, the easiest way to ensure that the majority of required packages
are installed is to add the packages for the items that you want to include to the list of packages that are 
downloaded for the base installation, for instance if you would like to include Ubuntu LXDE, try editing the template
file to locate and modify the following line:

----
packages=dialog,apt,apt-utils,iproute,inetutils-ping,vim,isc-dhcp-client,isc-dhcp-common,ssh,lsb-release,gnupg,netbase,ubuntu-keyring,lubuntu-desktop
----

Note that although this should help you to get started, it is quite possible that some packages may not get installed.
If you find that you are having trouble, but your container has been built. It is better to add packages from
within a chroot environment than from a running container.

On some Ubuntu systems, you may need to edit your /etc/fstab in order to accommodate the use of cgroups, which
are a special system mount much like /proc or /sys:

----
cgroup          /cgroup         cgroup  defaults        0       0
----

If you included the above line in your /etc/fstab, you will probably also need to create the directory and mount
it:

----
sudo mkdir /cgroup
sudo mount /cgroup
----

Once these steps have been taken, you should be able to start up your linux container from the command line:

----
lxc-start -n MyUbuntuContainer
----

This will start the boot process, which you will be able to watch within the console. Once it has finished loading, you
should be able to login as root using the password set up by your template. Usually the password is set to 'root' if you
are using one of the default templates.

IMPORTANT: You should now follow the instructions to <<qvd-vma-config,install and configure the QVD VMA>> and to <<qvd-serial-port-access,set up Serial Port Access>>.

When you have finished setting up your image, you can stop it from a different console or tty on the host system:

----
# lxc-stop -n MyUbuntuContainer
----

Note that when creating a container from one of the templates, network access will rely on the configuration used 
by the host system (if possible) and these resources will be shared between containers. This is not very secure, but is usually
fine to prepare an image for use within QVD, as the QVD infrastructure will provide its own configuration for the image
when it actually runs inside of QVD. However, if you are having trouble with your networking within the container, or you are
having trouble starting a container you may find that it is helpful to change how the container accesses the network. 
The usual approach is to create a bridge interface that can be used to route traffic to the virtual interface that is created 
when you start the container. Under Ubuntu, you can easily create a bridge interface by editing your network configuration file in /etc/network/interfaces:

----
# The primary network interface
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
       bridge_ports eth0
       bridge_fd 0
       bridge_maxwait 0
----

In this example, we have attached the usual ethernet port that connects the machine to the network
to the bridge, which will obtain an IP address using DHCP. You will need to bring up your bridge interface in
order to start using it:

----
# ifup br0
----

Now you will need to change the network parameters inside your LXC configuration for the container that you are
working on. Usually, this means that you will need to edit the file /var/lib/lxc/MyUbuntuContainer/config where
'MyUbuntuContainer' is the name of your container. Add the following lines to the configuration file:

----
lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
----

When the container is started, a virtual interface attached to the bridge will be created on the host system
and made available to the container once it has started.

SLES Image
~~~~~~~~~~
The following outline should guide you through preparing and setting up an LXC image
on a SLES 11 SP2 system. Note that due to limited support for LXC in previous versions of SLES,
QVD is only supported on versions above SLES 11 SP2.

SLES 11 SP2 includes its own template for building a base SLES container for LXC. This means that it is
possible to use the lxc-create command to quickly build a container. However, while the template will create
a functional container with a base installation of SLES, adding new packages or modifying the container can
quickly break its functionality under LXC. This makes it fairly important that you modify the template to
automatically install the packages that you need at build time. To proceed, open the template in an editor and
locate the following line:

----
# zypper --quiet --root $cache/partial-$arch --non-interactive --gpg-auto-import-keys in --auto-agree-with-licenses --no-recommends -t pattern base
----

This line is responsible for downloading and installing the base packages for SLES. You need to add the following lines
in order to create a template that includes the LXDE desktop and the necessary packages to install and configure QVD:

----
# zypper --quiet --root $cache/partial-$arch --non-interactive --gpg-auto-import-keys in --auto-agree-with-licenses --no-recommends -t pattern lxde
# zypper --quiet --root $cache/partial-$arch --non-interactive --gpg-auto-import-keys in --auto-agree-with-licenses --no-recommends postgresql-libs
# zypper --quiet --root $cache/partial-$arch --non-interactive --gpg-auto-import-keys in --auto-agree-with-licenses --no-recommends intlfonts-ttf
----

You should also add the following lines to install the QVD VMA packages:

----
# zypper ar http://download.opensuse.org/repositories/home:/qvd/SLE_11_SP2/ QVD
# zypper --quiet --root $cache/partial-$arch --non-interactive --gpg-auto-import-keys in --auto-agree-with-licenses --no-recommends qvd-vma
----

Now use the lxc-create command to generate a container:

----
# lxc-create -n MySLESContainer -t sles
----

Once these steps have been taken, you should be able to start up your linux container from the command line:

----
# lxc-start -n MyUbuntuContainer
----

This will start the boot process, which you will be able to watch within the console. Once it has finished loading, you
should be able to login as root using the password set up by your template. Usually the password is set to 'root' if you
are using one of the default templates.

When you have finished setting up your image, you can stop it from a different console or tty on the host system:

----
# lxc-stop -n MyUbuntuContainer
---- 

Note that sometimes configuring network parameters correctly in SLES can be relatively tricky and will require use of
the YAST configuration utility. If you are struggling to configure your network parameters, you may find it easier to
use a chrooted environment to complete the setup and configuration of your disk image. You can find out more about this
in the <<lxc-editing-di,next section>>.

IMPORTANT: You should now follow the instructions to <<qvd-vma-config,install and configure the QVD VMA>> and to <<qvd-serial-port-access,set up Serial Port Access>>.

Editing a DI
------------
At any point, you are able to quickly edit a DI. This means that you
can add new applications or remove existing applicaitons, or you can
implement new policies.

In order to edit an existing image, ensure that no users are
connected and using the image that you want to edit. Stop any Virtual
Machines that are currently making use of the DI, and block access to
them so that no users can connect and start up a virtual machine while
you are working on its underlying image.

KVM
~~~
Once the virtual machines have all been stopped, locate the DI file
that you wish to edit and run it within KVM in the following way:

----
# kvm -hda example.img -m 512
----

KVM will load a virtual machine and allow you to login as the user that 
you created when you installed the Operating System. You can now perform any
administration tasks as this user.

When you have completed any work on the DI, shut it down. You can
mark the virtual machines that require access to the image as
'unblocked' and allow them to start up again.

LXC
~~~
[[lxc-editing-di]]
Once the virtual machines have all been stopped, locate the LXC container
that you wish to edit in the Shared Storage within the 'basefs' directory.
Depending on your requirements, you can either chroot the directory and
work directly within it as needed or you can load it as an LXC instance.
Since loading an image into a separate LXC instance generally requires that
you configure networking properly and provide a configuration file it is generally
recommended that you rather attempt to perform modifications to an image
using a chroot.

The example below shows how you can use bind mounts and chroot to access an LXC
disk image to perform updates:

----
# mount -o bind /proc /var/lib/qvd/storage/basefs/1-image1.tgz/proc/
# mount -o bind /dev /var/lib/qvd/storage/basefs/1-image1.tgz/dev/
# mount -o bind /sys /var/lib/qvd/storage/basefs/1-image1.tgz/sys/
# chroot /var/lib/qvd/storage/basefs/1-image1.tgz
#
----

When you have finished making changes, remember to exit and unmount your bind mounts:

----
# exit
# umount /var/lib/qvd/storage/basefs/1-image1.tgz/proc
# umount /var/lib/qvd/storage/basefs/1-image1.tgz/dev
# umount /var/lib/qvd/storage/basefs/1-image1.tgz/sys
#
----

If everything has gone well, you should try to start a virtual machine that makes use of
this image to ensure that it starts up correctly. If it starts properly, mark the virtual
machines that require access to the image as 'unblocked' and allow them to start up again.

It is important that you test an LXC image after making changes to ensure that nothing has
changed a configuration to have direct access to hardware. Packages that have 'udev' as
a dependency can often result in trouble if you have not taken steps to prevent udev from
running.


VMA Hooks
---------
[[qvd-vma-hooks]]

Introduction
~~~~~~~~~~~~
VMA Hooks can be configured within an DI to trigger functionality within a Virtual Machine
when particular QVD related events take place. This allows you to automatically modify platform
behaviour to adapt the operating system to address particular client-related requirements and
to solve concrete problems.

Hooks are added as configuration entries within the VMA configuration file on the underlying DI.
Therefore, by editing the `/etc/qvd/vma.conf` file and adding an entry similar to the following:

....
  vma.on_action.connect = /etc/qvd/hooks/connect.sh
....

it is possible to ensure that the script `/etc/qvd/hooks/connect.sh` running on the virtual machine
will be executed everytime that a user connects to the virtual machine.

It is also possible for QVD to provision scripts with command line parameters that are specific to QVD,
such as:

  - State changes, actions, or the provisioning process that has triggered the call to the hook. 

  - Virtual machine properties defined in the administration database

  - Parameters generated by the authentication plugins. 

  - User connection parameters.

  - Parameters supplied by the client program. 

Hooks have their own log file, stored within `/var/log/qvd/qvd-hooks.log` on the virtual machine. This makes
it possible to view which hooks have triggered scripts to run and to debug any unusual behaviours.

Action Hooks
~~~~~~~~~~~~
Action Hooks are executed every time that a particular action begins.

If the hook fails with a non-zero error code, the action will be aborted. 

All action hooks receive these parameters. 

  - _qvd.vm.session.state_: Current X-Windows server state

  - _qvd.hook.on_action_: Action that triggers the hook.


connect
^^^^^^^
key: *vma.on_action.connect*

This hook is executed when a user starts (or resumes) an X-Windows session using the QVD Client. The script will execute
after all Provisioning Hooks have been triggered.

It also receives the following parameters by default:

  - _qvd.vm.user.name_ : the user's login. 

  - _qvd.vm.user.groups_ : groups that the user belongs to. 

  - _qvd.vm.user.home_ : the user's directory home. 

This hook is capable of receiving other connection parameters and any additional parameters assigned to the VM within the QVD-DB.

pre-connect
^^^^^^^^^^^
key: *vma.on_action.pre-connect*

This hook is executed when a user starts (or resumes) an X-Windows session from the QVD Client, with the difference 
that it will trigger a script to execute _before_ any of the Provisioning Hooks are implemented.

Parameters for pre-connect are the same as that for connect.

stop
^^^^
key: *vma.on_action.stop*

This hook is executed when an X-Windows session receives a request to be closed. This behaviour usually occurs when the VMA
recieves such a request from the QVD-WAT or the QVD CLI Administration Utility.

There are no additional parameters for this hook.

suspend
^^^^^^^
key: *vma.on_action.suspend*

This hook is executed when an X-Windows session is suspended. This usually happens if a user closes the QVD Client application.

There are no additional parameters for this hook.

poweroff
^^^^^^^^
key: *vma.on_action.poweroff*

This hook is executed when the virtual machine is shut down. 

There are no additional parameters for this hook.

State Hooks
~~~~~~~~~~~
State Hooks are executed when changes within the X-Windows session take place. These hooks
will always receive the parameter _qvd.hook.on_state_ with the current X-Windows state. 

connected
^^^^^^^^^
key: *vma.on_state.connected*

This hook is executed once a connection has been succefully established between the QVD Client 
and the X-Windows server that runs in the virtual machine. 

suspended
^^^^^^^^^
key: *vma.on_state.suspended*

This hook executes once the user closes the QVD Client and the X-Windows session is in the 
suspended state.

stopped
^^^^^^^
key: *vma.on_state.disconnected*

This hook executes when the X-Windows session ends.

Provisioning Hooks
~~~~~~~~~~~~~~~~~~
Provisioning Hooks recieve the same parameters that are available to the 'connect' Action Hook.

add_user
^^^^^^^^
key: *vma.on_provisioning.add_user*

When a user is connected for first time, if the user still does not exist, a new account is created 
for him in the virtual machine.

By default the account is created with the `useradd` command.

The hook 'add_user' allows an Administrator to modify this proccess and create the user account 
using an alternate method or script.

after_add_user
^^^^^^^^^^^^^^
key: *vma.on_provisioning.after_add_user*

Once the user account has been created, this hook can be used to perform additonal actions related to
setting up the user account within the virtual machine, such as the automatic configuration of an email
client or other similar tasks.

mount_home
^^^^^^^^^^
key: *vma.on_provisioning.mount_home*

By default, QVD mounts the first partition of the device configured with the entry "vma.user.home.drive" 
on the directory "/home" where the user's home directory is created (by the hook 'add_user'). Should this 
partition not exist, it is created on the fly.

With this hook it is possible to change this process so that some other behaviour takes place instead, such
as mounting a "/home" directory from an NFS server.


Operational Procedures
======================
[[ops]]
[partintro]
--
In this part of the manual, we will cover topics related to day-to-day operational procedures, such as
backups and logging, along with some of the more frequently used commands used within the QVD in order
to control access to a server node or virtual machine while performing basic administrative tasks.
--

Backups
-------

Backing up QVD-DB
~~~~~~~~~~~~~~~~~
Since QVD makes use of the open-source PostgreSQL database in order to handle its database requirements,
backing up your QVD data can be achieved using standard PostgreSQL backup and restore commands.

To backup your database, you can simply run the following command to output the database content to file:

----
# sudo su - postgres
# pgdump qvd > QVDDatabase.sql
----

Restoring the database is as simple as piping the SQL content back into the pgsql client:

----
# sudo su - postgres
# pgsql qvd < QVDDatabase.sql
----

PostgreSQL also gives you the ability to pipe content from one database into another, making it
relatively simple to replicate the database:

----
# pgdump -h host1 qvd | pgsql -h host2 qvd
----

For more complex backup requirements, refer directly to the PostgreSQL documentation at
http://www.postgresql.org/docs/8.3/static/backup-dump.html for more information.

Remember that once you have dumped your database to file, the file should be backed up following your
usual backup strategy.

Note that you may also find that it is useful to backup your database configuration files, so that
if you need to reinstall and configure your database you are able to do so quickly and with your 
configuration data at hand. On Ubuntu systems, these files are usually located at /etc/postgresql/8.4/main.
On SuSE Linux systems, you will be able to find these files at /var/lib/pgsql/data.

Backing up Shared Storage
~~~~~~~~~~~~~~~~~~~~~~~~~
All of QVD's disk images, user home data and overlay data is usually stored in some form of Shared 
Storage facility accessible using a network filesharing protocol like NFS, GFS or OCFS2. Although 
particular data, such as the overlay data and images stored within the 'staging' directory, are 
not critical during disaster recovery, we recommend that this data is backed up alongside active
disk images and user home data if possible.

Understanding how files are stored within the Shared Storage accessed by QVD will help you to plan
a reasonable backup strategy. Please refer to the section titled <<storage_folders,Shared Storage>> for
more information.

Backing up Configuration Files
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Since the majority of QVD's configuration data is stored within the database and QVD's configuration
files are relatively simple to create, they are not usually considered to have high-priority within
a backup strategy. Nonetheless, for nearly all components within the QVD infrastructure, configuration
files are stored within /etc/qvd. Note that all QVD Server Nodes should have identical configuration
files, so only one copy needs to be stored.

Logging
-------

Database Logs
~~~~~~~~~~~~~
Since QVD makes use of the open-source PostGreSQL database, logging options are controlled by editing
the PostGreSQL configuration files. To change logging parameters, please refer to the PostGreSQL 
documentation at: http://www.postgresql.org/docs/8.3/static/runtime-config-logging.html

On Ubuntu, PostGreSQL keeps database logs in: /var/log/postgresql/. On SuSE, PostGreSQL keeps database 
logs in: /var/lib/pgsql/data/pg_log.

TIP: If you are new to PostGreSQL you might find the PGAdmin tool handy, particularly for monitoring server
status, logs and transactions. You can find out more about the tool at http://www.pgadmin.org/

QVD Server Node Logs
~~~~~~~~~~~~~~~~~~~~
QVD Server Nodes also keep their own log files. These are usually located at /var/log/qvd.log.
Log output format and logging facilities are controlled using the Perl logging module
http://search.cpan.org/~mschilli/Log-Log4perl-1.36/lib/Log/Log4perl.pm[log4perl]. Configuration
of the logging within the QVD can be controlled by setting various configuration parameters. These
are covered in more depth in <<config_logging, the section on Log Configuration>>.

QVD Virtual Machine Logs 
~~~~~~~~~~~~~~~~~~~~~~~~
The QVD VMA is installed within the disk image that is used by each virtual machine as it starts up.
The QVD VMA will send log output to its own log file at /var/log/qvd.log within the virtual machine.
Remember that content, outside of user data, written to disk within a virtual machine will make use
of the overlay facility provided by QVD.

To review log data written to file within a virtual machine, if possible you should access the running 
virtual machine via the console:

----
# qvd-admin.pl vm console -f id=1
----

If you are using LXC virtualization, it is often easier to access log files directly from the parent
QVD Server Node. Remember that for a running environment, its filesystem is constructed as it is started
and mounted into /var/lib/qvd/storage/rootfs. This means that for any virtual machine, it is possible
to directly view the log files from the parent host:

----
# tail /var/lib/qvd/storage/rootfs/1-fs/var/log/qvd.log
----

QVD also stores a backup of the overlay for any LXC virtual machine that fails to start properly. These
backups can be accessed in /var/lib/qvd/storage/overlayfs and are usually prefixed with 'deleteme-' and are
named following a similar convention to that followed for the naming of successful overlays. See <<overlayfs,overlayfs>>
for more information.

Commonly Used Commands
----------------------
When performing regular administration tasks, it may be cumbersome working with the QVD-WAT, particularly if
you need to script behaviours. For systems administrators, we include this section to provide examples for some
of the more commonly used commands that can be issued using the QVD CLI Admin Utility. If you need more guidance
on the commands available to you, please refer back to the <<qvd-admin-cli, chapter where we discuss the QVD CLI Admin Utility>>
in more detail.

QVD Server Node Administration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
When performing maintenance on a Server Node, it is common to require that users are blocked from accessing
the Server Node while you are working. This can be easily achieved by using the QVD CLI Admin Utility:

----
# qvd-admin.pl host block -f name='qvd*'          #block access on any server node host with name beginning with 'qvd'
# qvd-admin.pl host list -f address='192.168.0.2' #list the details for any host with IP Address '192.168.0.2'
----

Once you have finished performing maintenance, remember to unblock the host:

----
# qvd-admin.pl host block -f name='qvd*'          #allow access on any server node host with name beginning with 'qvd'
----

It can also be useful to check QVD Server Node Host Counters to monitor access to a Server Node:

----
# qvd-admin.pl host counters
Id Name      HTTP Requests  Auth attempts Auth OK NX attempts NX OK Short sessions
__________________________________________________________________________________
1  qvd-test3 168            84            84      77          77    65            
2  qvd-test4 275            122           118     109         109   52
----

VM Administration
~~~~~~~~~~~~~~~~~
When performing maintenance on Virtual Machines or when updating images, it is often the case that
you will need to prevent access to a group of users, or to force them to disconnect. Filters can be
used to control how actions are effected. The following examples provide some common usage of the
QVD CLI Admin Utility to control virtual machines while you are performing maintenance:

----
# qvd-admin.pl vm block -f user_id=5         #block access on any VM where the user has an id=5
# qvd-admin.pl vm unblock -f name=test       #unblock access on any VM where the name=test
# qvd-admin.pl vm disconnect_user -f id=23   #force disconnect the user on VM with id=23
# qvd-admin.pl vm start -f id=1              #manually start the VM with id=1
# qvd-admin.pl vm stop -f osf_id=2           #manually stop all VMs using the OS Flavour with id=2
# qvd-admin.pl vm del -f user_id=5           #delete all Vms for the user with id=5
----

//Information for SLES Users
//==========================

//Installation notes
//------------------


//[bibliography]
//Bibliography
//============
//The bibliography list is a style of AsciiDoc bulleted list.

//[bibliography]
//- [[[taoup]]] Eric Steven Raymond. 'The Art of Unix
//  Programming'. Addison-Wesley. ISBN 0-13-142901-9.
//- [[[walsh-muellner]]] Norman Walsh & Leonard Muellner.
//  'DocBook - The Definitive Guide'. O'Reilly & Associates. 1999.
//  ISBN 1-56592-580-7.


[glossary]
Glossary
========
//Glossaries are optional. Glossaries entries are an example of a style
//of AsciiDoc labeled lists.

[glossary]

QVD::
  The Quality Virtual Desktop, a set of server components and a client application that provides remote virtual desktop access to users.

QVD Client::
  A modified NX Client capable of connecting to a Virtual Machine running on a QVD Server Node. The client is available for Linux and Windows operating systems.

QVD-DB::
  The QVD database. This is installed on top of a PostgreSQL RDBM Server. All of the server-side components within the QVD infrastructure rely on the database
  to communicate with each other and to implement functionality.

QVD Server Node::
  A host that is running the QVD Server Node components, including the QVD Node daemon, the L7R daemon and the HKD. 
  Usually there are multiple QVD Server Nodes within a typical deployment. The Virtual Machines that the QVD Client accesses
  run on different QVD Server Nodes.

QVD-WAT::
  The QVD Web Administration Tool. This is a web-based GUI that allows an Administrator to configure and monitor the running of the QVD environment.

QVD CLI Administration Utility::
  A Perl script that provides a command line interface with which the QVD environment can be monitored and managed.

L7R::
  The Layer-7 Router which acts as the broker for all QVD Client connections. This is a QVD Server Node daemon. It is responsible for authenticating users
  and routing client requests to the appropriate Server Node running the Virtual Machine for an authenticated user. It also monitors session status.

HKD::
  The House Keeping Daemon is a QVD Server Node daemon. It is responsible for starting and stopping Virtual Machines and for performing virtual machine health
  checking.

QVD Node::
  The QVD Node is a simple QVD Server Node daemon that is used to manage the run states of the L7R and HKD.

VMA::
  The Virtual Machine Agent is a QVD component that runs inside of a virtual machine to facilitate client connectivity to the virtual desktop and that is responsible
  for listening to management requests sent by the HKD. It also provides a number of 'hooks' that allow an Administrator to customize behaviours within the virtual machine.

VMA Hook::
  A facility within the VMA to trigger other functionality (usually through the use of scripts) within the virtual machine, based on particular state changes within the virtual machine.

Virtual Machine::
  A Virtual Machine is a virtualized system running on top of a base Operating System. Usually the virtualized system loads an OSF for the purpose of running a virtual operating system.

OSF::
  An Operating System Flavour is loaded into any number
  of virtual machines on a QVD Server Node in order to serve a virtual desktop to a client. The OSF is usually installed into QVD along with particular runtime parameters
  such as the amount of system memory that should be available to it.

DI::
  A Disk Image is a qcow2 image that has been created as a virtual disk containing an installed operating system. This image is then associated to an OSF.

User::
  Person using the QVD service, usually connected using the QVD Client.

Administrator::
  A user who has permission to access the management platform, usually via the QVD-WAT or through the QVD CLI Administration Utility

Session::
  The period that a user is actually connected to a virtual machine.
  
KVM::
  Kernel Virtual Machine. This is a hypervisor that is installed into the Linux Kernel to achieve type-1 virtualization. QVD makes use of KVM in order to run the virtual machines required to serve virtual desktops to end users.

LXC::
  Linux Containers. This is a virtualization technology that is included in the Linux Kernel. It uses a similar approach to the standard Linux 'chroot' command, but provides a greater degree of separation of resources. QVD can make use of LXC instead of KVM in order to run the virtual machines required to serve virtual desktops to end users. LXC virtualization is a lot less resource intensive, allowing you to take better advantage of existing hardware to service more users.


//[index]
//Index
//=====
